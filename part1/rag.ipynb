{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7345b45e",
      "metadata": {
        "id": "7345b45e"
      },
      "source": [
        "# Rebuilding WALL¬∑E's Memories with RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22cd5f78",
      "metadata": {
        "id": "22cd5f78"
      },
      "source": [
        "One ordinary Earth-cleaning afternoon, WALL¬∑E climbed a pile of old iPhones trying to rescue a Rubik's Cube."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XCC6GVJIGi58",
      "metadata": {
        "id": "XCC6GVJIGi58"
      },
      "source": [
        "<img src=\"https://compote.slate.com/images/17bdccdd-d8c9-44e6-b7f8-96f03ca50b33.jpeg?crop=1560%2C1040%2Cx0%2Cy0\" width=\"400\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LZtwQnovH_32",
      "metadata": {
        "id": "LZtwQnovH_32"
      },
      "source": [
        "A pigeon startled him. He slipped.\n",
        "\n",
        "**CRASH.**\n",
        "\n",
        "When WALL¬∑E woke up, something was wrong..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jhnZj53BDYxw",
      "metadata": {
        "id": "jhnZj53BDYxw"
      },
      "source": [
        "```\n",
        "\\>>> SYSTEM BOOTING...  \n",
        "\\>>> WALL-E unit #700X  \n",
        "\\>>> STATUS: üü• MEMORY CORRUPTED\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QcTBt5cyHLS0",
      "metadata": {
        "id": "QcTBt5cyHLS0"
      },
      "source": [
        "<img src=\"https://wp-cdn.fortect.com/uploads/2023/10/20111547/BSOD-Memory-Management-1024x536.webp\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d63099b",
      "metadata": {
        "id": "4d63099b"
      },
      "source": [
        "Oh no... The fall seemed pretty bad. Let's try asking him a couple of questions:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a93c71bc",
      "metadata": {
        "id": "a93c71bc"
      },
      "source": [
        "> üßë‚Äçüîß: It's okay, buddy. You took a pretty bad fall. Let‚Äôs try something simple. Who are you?  \n",
        "ü§ñ: I... I do not know. Memory blocks missing.  \n",
        "üßë‚Äçüîß: Hmm. Okay. Let‚Äôs try this... Do you remember EVE?  \n",
        "ü§ñ: E...V...E... error. No match found in memory banks. Who... is EVE?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XAAn1xO8IrBt",
      "metadata": {
        "id": "XAAn1xO8IrBt"
      },
      "source": [
        "üíî WALL¬∑E has lost all his memories..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee8e6ff9",
      "metadata": {
        "id": "ee8e6ff9"
      },
      "source": [
        "But wait! We found an ancient relic in a dusty old USB: **the original WALL¬∑E movie script! üìù**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cece8a6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cece8a6b",
        "outputId": "6e487289-3ad3-47db-df55-6e7eb5abe086",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "# Obtain the script! We are nice enough to locate and prepare it for you\n",
        "!curl -L \"https://assets.scriptslug.com/live/pdf/scripts/wall-e-2008.pdf?v=1729115058\" -o walle_script.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xRU3gV0JIfZE",
      "metadata": {
        "id": "xRU3gV0JIfZE"
      },
      "source": [
        "<img src=\"https://i.etsystatic.com/39233251/r/il/6c8e18/5323736276/il_fullxfull.5323736276_k62y.jpg\" width=\"300\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GkWPFk5pD83b",
      "metadata": {
        "id": "GkWPFk5pD83b"
      },
      "source": [
        "Good news! We can use this script to rebuild WALL¬∑E's memories using **Retrieval Augmented Generation** powered by **LangChain**.\n",
        "\n",
        "This will allow us to:\n",
        "- Load the original script\n",
        "- Break it into memory-safe chunks\n",
        "- Search relevant fragments when a question is asked\n",
        "- Use a language model to reconstruct answers\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ce4cfbe",
      "metadata": {
        "id": "5ce4cfbe"
      },
      "source": [
        "## What is Retrieval Augmented Generation (RAG)?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43ff2ea9",
      "metadata": {
        "id": "43ff2ea9"
      },
      "source": [
        "Large language models like GPT are powerful, but they don't have access to your custom data ‚Äî like WALL¬∑E's movie script ‚Äî unless you give it to them.\n",
        "\n",
        "**RAG (Retrieval-Augmented Generation)** is a way to augment an LLM with external knowledge dynamically.\n",
        "\n",
        "It works like this:\n",
        "1. When you ask a question, we retrieve relevant documents from a knowledge base (like pieces of a movie script).\n",
        "2. These documents are passed along with your question to the LLM, which then uses both to generate an informed response.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bzNwPP-LBW8",
      "metadata": {
        "id": "9bzNwPP-LBW8"
      },
      "source": [
        "Below is a more technical defintion:\n",
        "\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20250210184749053767/What-is-RAG_.webp\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JTv6s7T7K2jk",
      "metadata": {
        "id": "JTv6s7T7K2jk"
      },
      "source": [
        "**Why is this useful?**\n",
        "- You don‚Äôt have to fine-tune a model.\n",
        "- You can update data without retraining.\n",
        "- It keeps answers grounded in known sources.\n",
        "\n",
        "In this workshop, we‚Äôll build a RAG pipeline to help WALL¬∑E recall information from his script ‚Äî step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7OIhK1jBLxI5",
      "metadata": {
        "id": "7OIhK1jBLxI5"
      },
      "source": [
        "## What is LangChain?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FhYucoKGLzMW",
      "metadata": {
        "id": "FhYucoKGLzMW"
      },
      "source": [
        "We‚Äôve talked about what RAG is ‚Äî but how do we actually *build* a system that can retrieve documents and talk like WALL¬∑E? That‚Äôs where **LangChain** comes in."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IjAYeYMiL6ls",
      "metadata": {
        "id": "IjAYeYMiL6ls"
      },
      "source": [
        "LangChain is an open-source framework that connects together all distinctive parts in an AI app:\n",
        "- The **LLM** (e.g., OpenAI's GPT-4)\n",
        "- The **retriever** (e.g., a vector store to search memory)\n",
        "- The **embedding model** (to turn text into numeric form)\n",
        "- The **document loaders** (like PDFs, websites, or APIs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oRD2aXDdMEiQ",
      "metadata": {
        "id": "oRD2aXDdMEiQ"
      },
      "source": [
        "Instead of writing the code that integrates each component, LangChain gives us modular tools and pre-built chains to make everything talk to each other."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T0P6_c_cMmxb",
      "metadata": {
        "id": "T0P6_c_cMmxb"
      },
      "source": [
        "In this project, LangChain will help us:\n",
        "- Load the WALL¬∑E script\n",
        "- Split it into smaller chunks\n",
        "- Generate vector embeddings\n",
        "- Store and search those embeddings\n",
        "- Feed context to the LLM and return answers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf6abbc1",
      "metadata": {
        "id": "cf6abbc1"
      },
      "source": [
        "## üöÄ Step 0: Fire Up WALL¬∑E‚Äôs Core Systems (Environment Setup)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "882afcae",
      "metadata": {
        "id": "882afcae"
      },
      "source": [
        "Before we can help WALL¬∑E remember anything, we need to prepare the systems that simulate his brain.\n",
        "\n",
        "**Make sure you have a working [OpenAI API Key](https://platform.openai.com/account/api-keys) for the LLM and embedding model access.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f68c7e27",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f68c7e27",
        "outputId": "fffa0787-56b6-44ec-bb6b-658883fac0af",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "# Install the main langchain package\n",
        "!pip install --quiet --upgrade langchain\n",
        "\n",
        "# Install the main LangChain library and its key components required for our project\n",
        "!pip install --quiet --upgrade langchain-core langchain-text-splitters langchain-community langgraph langchain-openai\n",
        "\n",
        "# Install other dependencies to work with PDFs and transformers\n",
        "!pip install --quiet --upgrade pypdf sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10a463bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10a463bb",
        "outputId": "035ac3a8-115a-4fff-ccbf-cfc757ff0214"
      },
      "outputs": [],
      "source": [
        "# Load OpenAI API key\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e06afb5",
      "metadata": {
        "id": "8e06afb5"
      },
      "outputs": [],
      "source": [
        "# Set up embeddings model\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb586131",
      "metadata": {
        "id": "bb586131"
      },
      "outputs": [],
      "source": [
        "# Set up in-memory vector store\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8485d41b",
      "metadata": {
        "id": "8485d41b"
      },
      "outputs": [],
      "source": [
        "# Set up chat model\n",
        "from langchain.chat_models import init_chat_model\n",
        "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63b65363",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63b65363",
        "outputId": "4623a8b7-0a72-4b70-cf61-b6792d243829"
      },
      "outputs": [],
      "source": [
        "assert embeddings is not None, \"‚ùå Embeddings model uninitialized..\"\n",
        "assert vector_store is not None, \"‚ùå Vector store uninitialized.\"\n",
        "assert llm is not None, \"‚ùå Language model uninitialized.\"\n",
        "\n",
        "print(\"‚úÖ You are good to go!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Rz1Oc_SgM0vv",
      "metadata": {
        "id": "Rz1Oc_SgM0vv"
      },
      "source": [
        "**üõ†Ô∏è Want to customize?**\n",
        "\n",
        "What if I'm not a fan of OpenAI? Am I locked into using their embeddings and chat models?\n",
        "\n",
        "Of course not! LangChain is really flexible on this. You can easily swap out components to fit your needs or preferences:\n",
        "\n",
        "- Try different **LLMs** like Anthropic Claude, Cohere, or Mistral\n",
        "- Use different **vector stores** like FAISS, Pinecone, or Chroma\n",
        "- Run models **locally** or in the **cloud**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wNeLkQICPevt",
      "metadata": {
        "id": "wNeLkQICPevt"
      },
      "source": [
        "In fact, the only parts of the code you will need to modify are the initial setup cells above where we define the embedding model, vector store, and LLM. The rest of the pipeline ‚Äî loading, splitting, retrieving, generating ‚Äî will work just the same.\n",
        "\n",
        "This is one thing I really love about langchain: how *‚Äúplug-and-chug‚Äù* it is."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tsKD5lfjPcJd",
      "metadata": {
        "id": "tsKD5lfjPcJd"
      },
      "source": [
        "üìö Check out the [LangChain docs](https://docs.langchain.com/) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4511198c",
      "metadata": {
        "id": "4511198c"
      },
      "source": [
        "## üìú Step 1: Load the Memory Archive (Document Loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7c601d2",
      "metadata": {
        "id": "d7c601d2"
      },
      "source": [
        "Now that we have our basic setup, we‚Äôll start by loading the WALL¬∑E script from the PDF file that you (hopefully) have downloaded from running a previous cell. This will become the ‚Äúmemory source‚Äù from which WALL¬∑E can later reconstruct his thoughts.\n",
        "\n",
        "LangChain provides a `PyPDFLoader` that extracts text from each page of the script, returning it as a list of documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c283a8d5",
      "metadata": {
        "id": "c283a8d5",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load the PDF script into WALL¬∑E's recovery core\n",
        "loader = PyPDFLoader(\"walle_script.pdf\")\n",
        "pages = []\n",
        "async for page in loader.alazy_load():\n",
        "    pages.append(page)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Sjah3MPYJ5Co",
      "metadata": {
        "id": "Sjah3MPYJ5Co"
      },
      "source": [
        "```\n",
        "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 35% ‚Äî Locating movie script‚Ä¶  \n",
        "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë] 75% ‚Äî Found 1 source: `walle_script.pdf`  \n",
        "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% ‚Äî Script Loaded ‚úÖ\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L97AzAkAQ1OX",
      "metadata": {
        "id": "L97AzAkAQ1OX"
      },
      "source": [
        "Let‚Äôs inspect a sample page to check if the script is actually loaded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3392c24d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3392c24d",
        "outputId": "fb0a4292-56eb-4539-d390-2bc9569a49af"
      },
      "outputs": [],
      "source": [
        "print(f\"Total pages loaded: {len(pages)}\\n\")\n",
        "\n",
        "# Skipping the title page (pages[0])\n",
        "page_num = 1\n",
        "print(f\"{'='*40}\")\n",
        "print(f\"üìÑ Page {page_num} Metadata\")\n",
        "print(f\"{'-'*40}\")\n",
        "print(pages[page_num].metadata)\n",
        "\n",
        "print(f\"\\n{'='*40}\")\n",
        "print(f\"üìú Page {page_num} Content\")\n",
        "print(f\"{'-'*40}\")\n",
        "print(pages[page_num].page_content)\n",
        "print(f\"{'='*40}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ce1b7f8",
      "metadata": {
        "id": "5ce1b7f8"
      },
      "source": [
        "Looks good!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2_J36D5LTdoR",
      "metadata": {
        "id": "2_J36D5LTdoR"
      },
      "source": [
        "<img src=\"https://www.iamag.co/wp-content/uploads/2018/02/cover-walle.jpg\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_gBwg4XKRna6",
      "metadata": {
        "id": "_gBwg4XKRna6"
      },
      "source": [
        "Each page includes both the **text content** and **metadata** like page number, title, author, etc.\n",
        "\n",
        "This is useful for debugging, understanding file structure, or even filtering specific pages. But for our purposes, we don't have to worry too much about it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C1uKNWLcRLJ_",
      "metadata": {
        "id": "C1uKNWLcRLJ_"
      },
      "source": [
        "## ‚úÇÔ∏è Step 2: Break the Script into Memory-Safe Chunks (Text Splitter)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56972087",
      "metadata": {
        "id": "56972087"
      },
      "source": [
        "What happens if we try to give WALL¬∑E the **entire** script all at once? We gently place the documents into his input slot.  \n",
        "\n",
        "> Beep... Whirr... BZZZT...  \n",
        "> Eeee‚Äì...ERR‚Äì...üí•  \n",
        "\n",
        "Uh oh... Turns out WALL¬∑E‚Äôs memory unit has a limited space, just like most language models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SbFUB32yUTxz",
      "metadata": {
        "id": "SbFUB32yUTxz"
      },
      "source": [
        "Language models can only ‚Äúsee‚Äù a fixed number of tokens at a time, known as the **context window**.  \n",
        "\n",
        "For example, even powerful models like GPT-4 have a context limit (e.g., 8k, 32k, or 128k tokens depending on the variant). If you try to input more than that, the model will ignore or truncate the excess."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9733698e",
      "metadata": {
        "id": "9733698e"
      },
      "source": [
        "To help him process this massive amount of text in a manageable way, we need to:\n",
        "- Split the script into smaller **chunks**\n",
        "- Overlap these chunks to maintain **overall context**\n",
        "- Track their **original position** in the document"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qw8ik8EPT7vy",
      "metadata": {
        "id": "qw8ik8EPT7vy"
      },
      "source": [
        "We‚Äôll use LangChain‚Äôs `RecursiveCharacterTextSplitter`, which intelligently breaks documents based on structure (paragraphs, sentences, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68b50597",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68b50597",
        "outputId": "8037d65a-4952-4e0d-c6f5-8aa5d4d8b990"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # chunk size (characters)\n",
        "    chunk_overlap=200,  # chunk overlap (characters)\n",
        "    add_start_index=True,  # track index in original document\n",
        ")\n",
        "all_splits = text_splitter.split_documents(pages)\n",
        "\n",
        "print(f\"Split script into {len(all_splits)} sub-documents.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zex1vCt9VANA",
      "metadata": {
        "id": "zex1vCt9VANA"
      },
      "source": [
        "How this works:\n",
        "\n",
        "- The text splitter tries to split text at natural boundaries (e.g., paragraphs, then sentences, then characters).\n",
        "- We define the maximum number of characters (e.g. 1000) as the target chunk size.\n",
        "- We also define how much context from the previous chunk is retained in the next one (e.g. 200 characters).\n",
        "- If no good boundaries are found, it falls back to smaller units."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gsPLLoD1Vt-z",
      "metadata": {
        "id": "gsPLLoD1Vt-z"
      },
      "source": [
        "This smart splitting ensures that each piece of document remains semantically coherent and doesn‚Äôt randomly cut off mid-way."
      ]
    },
    {
      "cell_type": "raw",
      "id": "00cca40d",
      "metadata": {
        "id": "00cca40d",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "```\n",
        "‚öô Splitting long-term memory into chunks...\n",
        "\n",
        "[‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí] 5% ‚Äî Chunking‚Ä¶\n",
        "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí] 45% ‚Äî Overlapping with context\n",
        "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñí‚ñí‚ñí‚ñí‚ñí‚ñí] 75% ‚Äî Reducing noise\n",
        "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% ‚Äî 159 memory fragments generated ‚úÖ\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31d03c6f",
      "metadata": {
        "id": "31d03c6f"
      },
      "source": [
        "## üß≤ STEP 3: Upload to WALL¬∑E's Memory Module (Vector Store)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdb37ddc",
      "metadata": {
        "id": "bdb37ddc"
      },
      "source": [
        "WALL¬∑E doesn‚Äôt store memories like we do. It can't directly understand texts. Therefore, we need to convert each chunk of the movie script into an **embedding** ‚Äî a numerical vector that captures the semantic meaning of the text. These embeddings are created using a powerful transformer model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SDN-EaqAW-2M",
      "metadata": {
        "id": "SDN-EaqAW-2M"
      },
      "source": [
        "<img src=\"https://www.cs.cmu.edu/~dst/WordEmbeddingDemo/figures/fig5.png\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k4uZonsBW-UJ",
      "metadata": {
        "id": "k4uZonsBW-UJ"
      },
      "source": [
        "Once we have these vectors, we store them in a **vector store** ‚Äî a searchable database optimized for similarity-based retrieval."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74-NFCxIWZ4h",
      "metadata": {
        "id": "74-NFCxIWZ4h"
      },
      "source": [
        "LangChain abstracts these complicated ideas away nicely. It handles all the interfacing and heavy lifting, so we only need a single call to store the documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be8eea1b",
      "metadata": {
        "id": "be8eea1b"
      },
      "outputs": [],
      "source": [
        "document_ids = vector_store.add_documents(documents=all_splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AaVWwLA-WjU2",
      "metadata": {
        "id": "AaVWwLA-WjU2"
      },
      "source": [
        "Let‚Äôs print a few document IDs to confirm everything uploaded correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5097096",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5097096",
        "outputId": "ef3740a6-b0d6-40d4-b182-8ed7aa6775ff"
      },
      "outputs": [],
      "source": [
        "print(document_ids[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RMGu7jlrWrqp",
      "metadata": {
        "id": "RMGu7jlrWrqp"
      },
      "source": [
        "These unique IDs can be used to reference, update, or delete specific documents."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f07fb357",
      "metadata": {
        "id": "f07fb357"
      },
      "source": [
        "Now that WALL¬∑E‚Äôs memory has been fully indexed, we can start asking him questions.\n",
        "\n",
        "But before that, we need to create the actual ‚Äúthought pipeline‚Äù, basically the RAG chain that:\n",
        "1. Accepts a question\n",
        "2. Searches the vector store for relevant memory chunks\n",
        "3. Feeds those chunks to the language model\n",
        "4. Returns a contextual, informed answer\n",
        "\n",
        "We are almost there! Just need to connect the dots together."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cccca32",
      "metadata": {
        "id": "0cccca32"
      },
      "source": [
        "## ü§ñ Step 4: Reconstruct Thoughts ‚Äì Create the RAG Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ecc9f17",
      "metadata": {
        "id": "0ecc9f17"
      },
      "source": [
        "WALL¬∑E‚Äôs memory fragments are now embedded, indexed, and stored. It‚Äôs time to bring him back to life."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1MeRVVaIYU4L",
      "metadata": {
        "id": "1MeRVVaIYU4L"
      },
      "source": [
        "In a RAG pipeline, it‚Äôs not enough to just retrieve relevant documents ‚Äî what is the chat model supposed to do with them anyways? We need to *tell* the language model how to use them.\n",
        "\n",
        "This is where **prompting** comes in."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IBlvutzdYd34",
      "metadata": {
        "id": "IBlvutzdYd34"
      },
      "source": [
        "A **prompt** is the actual input string that gets sent to the language model. It usually includes:\n",
        "- The user‚Äôs question\n",
        "- Retrieved context (from the vector store)\n",
        "- Optional formatting or instructions (\"Answer concisely\", \"Use markdown\", etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XAiVBrTyYjL9",
      "metadata": {
        "id": "XAiVBrTyYjL9"
      },
      "source": [
        "Instead of hardcoding the prompt yourself, LangChain offers **LangChain Hub** ‚Äî a registry of ready-made prompt templates.\n",
        "\n",
        "We‚Äôll use a popular one for Retrieval-Augmented Generation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b33c467b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b33c467b",
        "outputId": "3a7062e5-a1c9-47d3-b22d-6a18c3885b32"
      },
      "outputs": [],
      "source": [
        "# Load a generic RAG-style prompt from LangChain Hub\n",
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DcrUS72uY8Mm",
      "metadata": {
        "id": "DcrUS72uY8Mm"
      },
      "source": [
        "By the way, here is how the actual prompt looks like.\n",
        "\n",
        "*üí° You can customize this later with your own prompt templates ‚Äî just make sure it contains placeholders for question and context.*\n",
        "\n",
        "```\n",
        "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ss2Invw8Zhn7",
      "metadata": {
        "id": "ss2Invw8Zhn7"
      },
      "source": [
        "Let's try asking a question and see if WALL¬∑E is able to spit out the correct answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "213af777",
      "metadata": {
        "id": "213af777"
      },
      "outputs": [],
      "source": [
        "# Ask WALL¬∑E a question\n",
        "question = \"Who is Eve?\"\n",
        "\n",
        "# Step 1: Retrieve relevant documents using similarity search\n",
        "retrieved_docs = vector_store.similarity_search(question)\n",
        "\n",
        "# Step 2: Combine their content into a single context block\n",
        "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "\n",
        "# Step 3: Plug question + context into the RAG prompt\n",
        "final_prompt = prompt.invoke({\n",
        "    \"question\": question,\n",
        "    \"context\": docs_content\n",
        "})\n",
        "\n",
        "# Step 4: Send the composed prompt to the LLM for response generation\n",
        "answer = llm.invoke(final_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "799f887e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "799f887e",
        "outputId": "943595d3-89e3-44f0-a1b4-9da8f297a491"
      },
      "outputs": [],
      "source": [
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fYSZkXk6Z6Cl",
      "metadata": {
        "id": "fYSZkXk6Z6Cl"
      },
      "source": [
        "Hooray!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Pi10kNrMODMY",
      "metadata": {
        "id": "Pi10kNrMODMY"
      },
      "source": [
        "<img src=\"https://davidswanson.wordpress.com/files/2009/02/wall-e.jpg\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d35e4471",
      "metadata": {
        "id": "d35e4471"
      },
      "source": [
        "## üåü Mission Complete: WALL¬∑E Remembers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db868ec9",
      "metadata": {
        "id": "db868ec9"
      },
      "source": [
        "You've just built a working **RAG system**!\n",
        "\n",
        "Let‚Äôs recap what we did:\n",
        "- Used LangChain to load a real-world document\n",
        "- Split it into reasonably sized chunks\n",
        "- Turned those into vector embeddings\n",
        "- Stored them in a searchable vector store\n",
        "- Queried it via an LLM to simulate memory reconstruction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76554bd9",
      "metadata": {
        "id": "76554bd9"
      },
      "source": [
        "This same pipeline can be adapted for:\n",
        "- Document Q&A systems\n",
        "- Chatbots with memory\n",
        "- Internal knowledge assistants\n",
        "- Customer support agents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5657b4b4",
      "metadata": {
        "id": "5657b4b4"
      },
      "source": [
        "Now that you‚Äôve helped WALL¬∑E recover, try loading your own documents ‚Äî and help something else remember. üíæü§ñ"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
